---
layout:     post
title:      LSTM原理学习
date:       2019-01-20 14:26:00
summary:    理解Lstm的输入以及输出和模型对应的维度
categories: jekyll pixyll
---

## LSTM原理

###时序数据
nn.LSTM()函数的参数：
- input_size: 表示输入的数据维度 每一个time_step 输入到lstm单元的维度实际上输入的数据size[batch_size,input_size]
- hidden_size: 表示输出的维度
- num_layers: 表示堆叠几层的LSTM，默认是1
- batch_first:True或者False,因为nn.lstm()接收的数据是(序列长度)，所以使用batch_first，我们可以把输入变成(batch,序列长度，输入维度) 否则就是(序列长度，batch,输入维度)
- bidirectional：双向lstm，也就是算两边  得到两倍的输出。

![](/images/1547967162.jpg)

这里我们定义了一个lstm模型，我们需要传入的参数是输入数据的维度28，lstm输出的维度是128，lstm的层数是2层以及输出类数为10。
在网络定义里面首先需要定义lstm，而长度为28的序列传入lstm之后输出也是28，而输入的维度是28，输出的维度由我们定义的128，最后我们只取输出的最后一个部分传入分类器求出分类概率。
out = out[:-1,:]通过这种方式，out的三个维度分别表示batch_size,序列长度和数据维度，所以中间的序列长度取-1，表示取序列中的最后一个数据，这个数据维度为128，再通过分类器，输出10个结果表示每种结果的概率。


### Example：An LSTM for Part-of-Speech Tagging词性标注
![](/images/lsdsohs.jpg)



### 推荐阅读：[Building RNNs is Fun with PyTorch and Google Colab](https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79).


> 总结： lstm等循环神经网络定义的单个lstm结构实际上就是多个神经元组成的，
